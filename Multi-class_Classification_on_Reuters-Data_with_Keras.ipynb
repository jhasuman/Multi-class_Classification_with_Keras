{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class Classification\n",
    "\n",
    "In this notebook we are going to build a deep learning model on **Reuters dataset in Keras**. Reuters datasets comes as package with Keras so we don't have to download it from any website.\n",
    "\n",
    "**About Dataset**: Data set is set of short newswires and their topics, published by Reuters in 1986. There are 46 different topics and every topics has minimum of 10 examples in the traning set.\n",
    "\n",
    "Dataset has been splited into traning and test with 8982 and 2246 examples respectively.\n",
    "\n",
    "**Type of Multi-class classification**: There are two types of Multi-class classification \n",
    "1. Single label multi-class classification : Each data point (examples in our case) will classify into only one category.  \n",
    "2. Multi label multi-class classification  : Each data point will calssify into more than one category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import the libraries \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the data\n",
    "from keras.datasets import reuters\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8982"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data checking\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2246"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 4,\n",
       " 37,\n",
       " 38,\n",
       " 309,\n",
       " 213,\n",
       " 349,\n",
       " 1632,\n",
       " 48,\n",
       " 193,\n",
       " 229,\n",
       " 463,\n",
       " 28,\n",
       " 156,\n",
       " 635,\n",
       " 11,\n",
       " 82,\n",
       " 14,\n",
       " 156,\n",
       " 635,\n",
       " 11,\n",
       " 82,\n",
       " 54,\n",
       " 139,\n",
       " 16,\n",
       " 349,\n",
       " 105,\n",
       " 462,\n",
       " 311,\n",
       " 28,\n",
       " 296,\n",
       " 147,\n",
       " 11,\n",
       " 82,\n",
       " 14,\n",
       " 296,\n",
       " 147,\n",
       " 11,\n",
       " 54,\n",
       " 139,\n",
       " 342,\n",
       " 48,\n",
       " 193,\n",
       " 3234,\n",
       " 361,\n",
       " 122,\n",
       " 23,\n",
       " 1332,\n",
       " 28,\n",
       " 318,\n",
       " 942,\n",
       " 11,\n",
       " 82,\n",
       " 14,\n",
       " 318,\n",
       " 942,\n",
       " 11,\n",
       " 82,\n",
       " 54,\n",
       " 139,\n",
       " 122,\n",
       " 7,\n",
       " 105,\n",
       " 462,\n",
       " 23,\n",
       " 349,\n",
       " 28,\n",
       " 296,\n",
       " 767,\n",
       " 11,\n",
       " 82,\n",
       " 14,\n",
       " 296,\n",
       " 767,\n",
       " 11,\n",
       " 54,\n",
       " 139,\n",
       " 342,\n",
       " 229,\n",
       " 162,\n",
       " 7,\n",
       " 48,\n",
       " 193,\n",
       " 55,\n",
       " 408,\n",
       " 28,\n",
       " 258,\n",
       " 557,\n",
       " 11,\n",
       " 82,\n",
       " 14,\n",
       " 196,\n",
       " 557,\n",
       " 11,\n",
       " 82,\n",
       " 54,\n",
       " 139,\n",
       " 162,\n",
       " 7,\n",
       " 105,\n",
       " 462,\n",
       " 55,\n",
       " 349,\n",
       " 28,\n",
       " 191,\n",
       " 968,\n",
       " 11,\n",
       " 82,\n",
       " 14,\n",
       " 191,\n",
       " 785,\n",
       " 11,\n",
       " 54,\n",
       " 139,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's findout how data train data looks like\n",
    "train_data[5] #The reuters data short news is a sequence of integer and every integer represents words from dictonery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can decode the data from integer to words but we are not going to do this here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Prepration\n",
    "# Since train data is in form of sequence of words so we need to vectorize it.\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=12000): #Because we had restricted the number of words 12000\n",
    "    results = np.zeros((len(sequences), dimension)) #Creating a zero vector of size sequences and dimension\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our Vectorized traning and test data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vectorizitation of labels\n",
    "#Keras has inbuild way of converting multi labels into vector form using utiles\n",
    "from keras.utils.np_utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Network formation\n",
    "#We are building three nureal layer network.\n",
    "#Number of hidden unit in hidden layer is 64 because out layer has 46 units. So if we are are using hidden unit less\n",
    "#than 46 information dorwpout might occur.\n",
    "#We have used relu as our activation function for hidden layers and softmax for output layer.\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential() #We are using sequential model because we are dealing with 2D tensor data\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(12000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model\n",
    "#We have used rmsprop as optmizier, categorical_crossentropy as loss function because it measures the distance between prediction and ground trouth\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Creating validation data set\n",
    "x_val = x_train[:1000]\n",
    "x_train = x_train[1000:]\n",
    "\n",
    "y_val = train_labels[:1000]\n",
    "y_train = train_labels[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 218us/step - loss: 2.5265 - acc: 0.5005 - val_loss: 1.7178 - val_acc: 0.6120\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s 206us/step - loss: 1.4382 - acc: 0.6906 - val_loss: 1.3430 - val_acc: 0.7110\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s 205us/step - loss: 1.0816 - acc: 0.7676 - val_loss: 1.1664 - val_acc: 0.7430\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 2s 206us/step - loss: 0.8526 - acc: 0.8202 - val_loss: 1.0861 - val_acc: 0.7560\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 2s 204us/step - loss: 0.6842 - acc: 0.8527 - val_loss: 0.9834 - val_acc: 0.7790\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 204us/step - loss: 0.5482 - acc: 0.8839 - val_loss: 0.9400 - val_acc: 0.8020\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 204us/step - loss: 0.4416 - acc: 0.9074 - val_loss: 0.9142 - val_acc: 0.8060\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s 205us/step - loss: 0.3553 - acc: 0.9251 - val_loss: 0.9474 - val_acc: 0.7910\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 2s 206us/step - loss: 0.2905 - acc: 0.9346 - val_loss: 0.8970 - val_acc: 0.8090\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 2s 206us/step - loss: 0.2433 - acc: 0.9436 - val_loss: 0.9132 - val_acc: 0.8110\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 2s 205us/step - loss: 0.2100 - acc: 0.9485 - val_loss: 0.9275 - val_acc: 0.8120\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 2s 204us/step - loss: 0.1804 - acc: 0.9514 - val_loss: 0.9163 - val_acc: 0.8100\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s 206us/step - loss: 0.1646 - acc: 0.9528 - val_loss: 0.9412 - val_acc: 0.8110\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s 205us/step - loss: 0.1491 - acc: 0.9558 - val_loss: 0.9741 - val_acc: 0.8120\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s 205us/step - loss: 0.1358 - acc: 0.9554 - val_loss: 0.9805 - val_acc: 0.8120\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 206us/step - loss: 0.1289 - acc: 0.9554 - val_loss: 1.0385 - val_acc: 0.8030\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 206us/step - loss: 0.1198 - acc: 0.9582 - val_loss: 1.0344 - val_acc: 0.7990\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s 204us/step - loss: 0.1179 - acc: 0.9572 - val_loss: 1.0599 - val_acc: 0.8040\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s 205us/step - loss: 0.1120 - acc: 0.9595 - val_loss: 1.1171 - val_acc: 0.7980\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s 206us/step - loss: 0.1094 - acc: 0.9593 - val_loss: 1.0863 - val_acc: 0.8070\n"
     ]
    }
   ],
   "source": [
    "#Traning the models in 20 epochs in batchsize 512\n",
    "history = model.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**We can see after epoch 15 the model is start overfitting. We can visualize through graph** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEPCAYAAABGP2P1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAQCQQjAAAFL+HwBll4ab4T4hAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faaeffa2450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's plot the loss and accuracy \n",
    "#Plotting loss \n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo')\n",
    "plt.plot(epochs, val_loss_values, 'b+')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above graph we can see, initially loss function has been decreasing with each epochs but after epochs 16 it has again started increasing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEPCAYAAABGP2P1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faafe1371d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's plot traning and validation accuracy\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo')\n",
    "plt.plot(epochs, val_acc_values, 'b+')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It seems network start overfitting after epochs 15.** *Which means we can get the maximum validation accuracy for epochs 15.*\n",
    "We can retrain our model for the getting maximum accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/14\n",
      "7982/7982 [==============================] - 2s 207us/step - loss: 2.5265 - acc: 0.5155 - val_loss: 1.6995 - val_acc: 0.6310\n",
      "Epoch 2/14\n",
      "7982/7982 [==============================] - 1s 180us/step - loss: 1.4099 - acc: 0.6954 - val_loss: 1.3108 - val_acc: 0.7090\n",
      "Epoch 3/14\n",
      "7982/7982 [==============================] - 1s 181us/step - loss: 1.0526 - acc: 0.7747 - val_loss: 1.1442 - val_acc: 0.7590\n",
      "Epoch 4/14\n",
      "7982/7982 [==============================] - 1s 181us/step - loss: 0.8343 - acc: 0.8262 - val_loss: 1.0465 - val_acc: 0.7660\n",
      "Epoch 5/14\n",
      "7982/7982 [==============================] - 1s 180us/step - loss: 0.6635 - acc: 0.8614 - val_loss: 0.9989 - val_acc: 0.7860\n",
      "Epoch 6/14\n",
      "7982/7982 [==============================] - 1s 179us/step - loss: 0.5254 - acc: 0.8899 - val_loss: 0.9531 - val_acc: 0.7970\n",
      "Epoch 7/14\n",
      "7982/7982 [==============================] - 1s 179us/step - loss: 0.4225 - acc: 0.9099 - val_loss: 0.9241 - val_acc: 0.8160\n",
      "Epoch 8/14\n",
      "7982/7982 [==============================] - 1s 180us/step - loss: 0.3439 - acc: 0.9262 - val_loss: 0.9049 - val_acc: 0.8160\n",
      "Epoch 9/14\n",
      "7982/7982 [==============================] - 1s 180us/step - loss: 0.2828 - acc: 0.9396 - val_loss: 0.9487 - val_acc: 0.8080\n",
      "Epoch 10/14\n",
      "7982/7982 [==============================] - 1s 181us/step - loss: 0.2377 - acc: 0.9461 - val_loss: 0.9026 - val_acc: 0.8240\n",
      "Epoch 11/14\n",
      "7982/7982 [==============================] - 1s 180us/step - loss: 0.2049 - acc: 0.9499 - val_loss: 0.9471 - val_acc: 0.8100\n",
      "Epoch 12/14\n",
      "7982/7982 [==============================] - 1s 180us/step - loss: 0.1809 - acc: 0.9515 - val_loss: 0.9979 - val_acc: 0.8070\n",
      "Epoch 13/14\n",
      "7982/7982 [==============================] - 1s 182us/step - loss: 0.1644 - acc: 0.9528 - val_loss: 0.9715 - val_acc: 0.8150\n",
      "Epoch 14/14\n",
      "7982/7982 [==============================] - 1s 180us/step - loss: 0.1481 - acc: 0.9548 - val_loss: 0.9890 - val_acc: 0.8140\n",
      "2246/2246 [==============================] - 0s 150us/step\n"
     ]
    }
   ],
   "source": [
    "# Let's retrain our model for epochs 15\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(12000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "#Compiling the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Traning the model\n",
    "model.fit(x_train, y_train, epochs=14 , batch_size=512, validation_data=(x_val, y_val))\n",
    "\n",
    "#Results of test data\n",
    "results = model.evaluate(x_test, test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still we can do some further experiement for increasing the accuracy because we are getting maximum accuracy at epochs 8.\n",
    "**You should try.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1400843709043806, 0.78584149601941433]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approch we are getting accuracy of 78%, which is not bad but accuracy can be increased. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some of the things which I have not mentioned like Why History, activation function etc. You can find about these details in binary calssification.** "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
